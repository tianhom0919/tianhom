---
title: About LLMs
description: About LLMs
authors:
  - eng-tian-hom
hide_table_of_contents: false
---

# Using LLMs

Large language models such as ChatGPT have proven to be very useful in software development, especially during debugging.

ChatGPT has been a lifesaver for software engineers as they can:

- paste error messages to ask where the error might be
- ask to clarify how a code snippet works
- explanations about fundamental concepts
- ask for code configuration

# The Problem

The biggest problem with LLMs is perhaps, [hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)). Sometimes the answers generated may look very convincing even though they are completely wrong. When programming, the hallucinated code can be recognized and caught quickly if it does not work. Problematic situations arise when the code seems to work, but contains bugs or security vulnerabilities that are harder to detect.

Another problem is that it is difficult for LLMs to "understand" larger projects. Usually in a large project, making changes in one file would require changes to several other files. There are token limits in ChatGPT. Once exceeded, older parts of the conversation or input will be forgotten. This means that LLMs are unable to generalize code, i.e. making minor changes for the requested functionality so that existing functions or components can be reused. This would result in the deterioration of the code base and decrease maintainability, as it generates a lot of repetitive code.

In this case, the responsibility is with the programmer that uses language models. Quoting [Brian Kerningham](https://en.wikipedia.org/wiki/Brian_Kernighan), the co-author of **The C Programming Language**:

> *“Everyone knows that debugging is twice as hard as writing a program in the first place. So if you're as clever as you can be when you write it, how will you ever debug it?”*
>
> - **Brian Kernighan**

Simply put, it is not worth programming code you don't understand since debugging would be twice as hard as programming. How can debugging be even possible when the programming is outsourced to an LLM, where the software developer does not understand the debugged code at all?

<!-- truncate -->